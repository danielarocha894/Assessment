# -*- coding: utf-8 -*-
"""dltHub_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fyy6kgdP5rI67WpoqEUtkaJbkNIe-ODq
"""

# -*- coding: utf-8 -*-
"""UTM Attribution Analysis - Working Student Data Task - COLAB VERSION"""

"""*üì¶ DEPENDENCIES INSTALLATION FOR COLAB*"""

!pip install dlt duckdb
!pip install plotly

"""*üß© LIBRARY IMPORTS*"""

import numpy as np  # Numerical computing and array operations
import pandas as pd  # Data manipulation and analysis
from datetime import datetime  # Date and time handling
import matplotlib.pyplot as plt  # Data visualization and plotting
import seaborn as sns  # Statistical data visualization
from sklearn.preprocessing import StandardScaler, OrdinalEncoder  # Data preprocessing utilities
from sklearn.decomposition import PCA  # Dimensionality reduction
from sklearn.cluster import KMeans  # Clustering algorithm
from sklearn.metrics import silhouette_score  # Cluster evaluation metric
import dlt  # Data pipeline creation library
import duckdb  # Analytical database for SQL queries
from collections import Counter  # For counting word frequencies
from google.colab import files  # Google Colab file upload utility
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots


print("‚úÖ All libraries imported successfully")

"""*üì§ DATA FILE UPLOAD*"""

print("üìÅ Please upload file")
uploaded = files.upload()  # Opens file upload dialog

# Get the uploaded filename
file_name = list(uploaded.keys())[0]  # Extract first uploaded file name
print(f"‚úÖ File '{file_name}' uploaded successfully")

"""*1. DATA LOADING AND INITIAL EXPLORATION*"""

print("üìä LOADING AND ANALYZING DATA...")

# Load the dataset from CSV file
df = pd.read_csv(file_name)

print("üìä DATASET STRUCTURE ANALYSIS")
print("=" * 50)
print(f"‚Ä¢ Dimensions: {df.shape[0]:,} rows √ó {df.shape[1]} columns")
print(f"‚Ä¢ Available columns: {list(df.columns)}")

# Analyze data types and identify missing values
print("\nüîç DETAILED DATA TYPE INFORMATION:")
df.info()  # Shows column data types, non-null counts, and memory usage

"""*2. DATA CLEANING AND PREPARATION*"""

print("\nüßπ STARTING DATA CLEANING AND PREPARATION...")

# Standardize column names to snake_case for better readability and processing
df = df.rename(columns={
    "Contact ID": "contact_id",
    "Field ID": "field_id",
    "Field Value": "field_value",
    "Contact Create date": "contact_created_at",
    "Contact Update date": "contact_updated_at",
    "Fields Title": "field_title",
    "Fields Type": "field_type",
    "Fields Update date": "field_updated_at",
    "Events Category": "event_category",
    "Events Create date": "event_created_at",
    "Events Hash": "event_hash",
    "Events ID": "event_id"
})

print("‚úÖ Column names standardized to snake_case convention")

# Convert date columns to proper DateTime format for temporal analysis
date_columns = [
    "contact_created_at",  # When contact was first created
    "contact_updated_at",  # When contact was last updated
    "field_updated_at",    # When field value was updated
    "event_created_at"     # When event occurred
]

# Convert each date column with error handling for invalid dates
for col in date_columns:
    df[col] = pd.to_datetime(df[col], errors="coerce")  # 'coerce' converts invalid dates to NaT

print("‚úÖ Date columns converted to DateTime format")

"""*3. DATA PROCESSING FOR ANALYSIS*"""

print("\nüîÑ PROCESSING DATA FOR ANALYSIS...")

# Create focused dataset for attribution analysis by selecting relevant columns
result = df[[
    "contact_id",          # Unique identifier for each contact
    "field_value",         # UTM source values and other field data
    "field_title",         # Title/description of the field
    "event_category",      # Type of event (click, view, etc.)
    "event_created_at",    # When event occurred
    "field_updated_at",    # When field was updated
    "contact_created_at",  # When contact was created
    "contact_updated_at"   # When contact was last updated
]].copy()  # Create independent copy to avoid modifying original

print(f"üìã Analysis dataset created with {result.shape[0]:,} records")

# Clean field values: replace long hash values with 'undefined' for better categorization
result["field_value"] = result["field_value"].apply(lambda x: "undefined" if pd.notnull(x) and len(str(x)) > 64 else x
    # Long hashes are likely system-generated IDs, not meaningful for analysis
)

# Create combined field that merges value and title for comprehensive analysis
result["field_title_value"] = result.apply(
    lambda row: f"{row['field_value']} - {row['field_title']}"
    if pd.notnull(row["field_value"]) and len(str(row["field_value"])) < 64
    else row["field_title"],  # Use only title if value is too long or missing
    axis=1  # Apply function row-wise across the DataFrame
)

print("‚úÖ Field values processed and combined field created")

"""*4. CONTACTS TABLE CREATION WITH AUTOMATIC GROUPING*"""

print("\nüéØ STARTING AUTOMATIC UTM SOURCE GROUPING...")

def find_common_prefixes(series, min_occurrences=2):
    """
    Identify common prefixes that repeat across field values
    to create meaningful categories for analysis.

    Args:
        series: Pandas Series containing field values
        min_occurrences: Minimum times a word must appear to be considered

    Returns:
        List of top 10 anchor words for grouping
    """
    all_values = series.dropna().astype(str)  # Remove NaN and convert to string
    # Extract all words from all values for frequency analysis
    all_words = []
    for value in all_values:
        words = value.split()  # Split string into individual words
        all_words.extend(words)  # Add to master word list

    word_counter = Counter(all_words)  # Count frequency of each word

    # Find anchor words that appear in multiple unique values
    anchor_words = []
    for word, count in word_counter.most_common(50):  # Check top 50 most common words
        if count >= min_occurrences and len(word) > 3:  # Filter for meaningful words
            # Verify this word appears in multiple unique values
            values_with_word = [v for v in all_values.unique() if word in v]
            if len(values_with_word) >= min_occurrences:
                anchor_words.append(word)
                return anchor_words[:10]  # Return top 10 anchor words

def group_by_prefix(value, anchor_words):
    """
    Group values by identified prefixes while preserving original UTMs
    for accurate attribution tracking.

    Args:
        value: Field value to categorize
        anchor_words: List of common words for grouping

    Returns:
        Categorized string value
    """
    value_str = str(value)

    # Always preserve original UTM parameters for accurate source tracking
    if 'utm' in value_str.lower():
        return value_str  # Keep original UTM values unchanged

    # For non-UTMs: find the most relevant anchor word
    for anchor in anchor_words:
        if anchor in value_str:
            return f"category_{anchor}"  # Create category based on anchor word

    # Fallback strategy: use first word or generic category
    words = value_str.split()
    if words:
        return f"category_{words[0]}"  # Use first word as category
    else:
        return "other_values"  # Default category for unclassifiable values

# Execute automatic pattern analysis on the combined field values
print("üìä ANALYZING VALUE PATTERNS...")
anchor_words = find_common_prefixes(result['field_title_value'])

print(f"üéØ IDENTIFIED ANCHOR WORDS ({len(anchor_words)}):")
for i, word in enumerate(anchor_words, 1):
    # Count how many values contain each anchor word
    count = sum(result['field_title_value'].astype(str).str.contains(word, case=False, na=False))
    print(f"  {i:2d}. '{word}' ‚Üí {count:>4} occurrences")

# Apply automatic grouping to all field values
print(f"\nüîÑ APPLYING AUTOMATIC GROUPING...")
result['field_title_grouped'] = result['field_title_value'].apply(
    lambda x: group_by_prefix(x, anchor_words)
)

"""*FINAL DATASETS CREATION*"""

print(f"\nüìä CREATING FINAL ANALYSIS DATASETS...")

# Dataset 1: Main contacts data with acquisition ranking
utm_data = (
    result
    .sort_values(by=["contact_id", "contact_created_at", "field_updated_at"])  # Sort chronologically
    .assign(acq_rank=lambda d: d.groupby("contact_id")["contact_created_at"]
            .rank(method="first", ascending=True).astype(int))  # Rank acquisitions per contact
)

# Dataset 2: Final contacts dataframe for analysis with grouped UTM sources
contacts_df = utm_data[[
    "contact_id",
    "contact_created_at",
    "field_title_grouped",  # This becomes our UTM source after grouping
    "acq_rank"              # Acquisition sequence number
]].rename(columns={"field_title_grouped": "utm_source"}).reset_index(drop=True)  # Rename for clarity

print(f"‚úÖ Contacts dataset created with {contacts_df.shape[0]:,} records")

# Dataset 3: Events table for engagement analysis
events_df = (
    result.loc[result["event_created_at"].notna(),  # Filter only records with event dates
               ["contact_id", "event_category", "event_created_at", "contact_created_at"]]
    .copy()  # Create independent copy
)

# Filter events to ensure they occur after contact creation (data quality check)
events_df = events_df[events_df["event_created_at"] >= events_df["contact_created_at"]]

# Sort events chronologically for each contact to analyze engagement sequence
events_df = events_df.sort_values(["contact_id", "event_created_at"]).reset_index(drop=True)

print(f"‚úÖ Events table created with {events_df.shape[0]:,} records")

"""*5. DATABASE LOADING AND ANALYSIS*"""

print("\nüíæ STARTING DATABASE LOADING...")

# Create dlt pipeline for loading data into DuckDB
pipeline = dlt.pipeline(
    pipeline_name="utm_attribution_colab",  # Pipeline identifier
    destination="duckdb",                   # Target database
    dataset_name="silver"                   # Database schema name
)

# Load contacts data into database with replace mode (overwrites existing)
pipeline.run(contacts_df, table_name="contacts", write_disposition="replace")
# Load events data into database
pipeline.run(events_df, table_name="events", write_disposition="replace")

print("‚úÖ Data successfully loaded into DuckDB!")

"""*6. ANALYSIS AND VISUALIZATION*"""

print(f"\nüìä CREATING FINAL ANALYSIS DATASETS...")

# Dataset 1: Main contacts data with acquisition ranking
utm_data = (
    result
    .sort_values(by=["contact_id", "contact_created_at", "field_updated_at"])  # Sort chronologically
    .assign(acq_rank=lambda d: d.groupby("contact_id")["contact_created_at"]
            .rank(method="first", ascending=True).astype(int))  # Rank acquisitions per contact
)

# Dataset 2: Final contacts dataframe for analysis with grouped UTM sources
contacts_df = utm_data[[
    "contact_id",
    "contact_created_at",
    "field_title_grouped",  # This becomes our UTM source after grouping
    "acq_rank"              # Acquisition sequence number
]].rename(columns={"field_title_grouped": "utm_source"}).reset_index(drop=True)  # Rename for clarity

print(f"‚úÖ Contacts dataset created with {contacts_df.shape[0]:,} records")

# Dataset 3: Events table for engagement analysis
events_df = (
    result.loc[result["event_created_at"].notna(),  # Filter only records with event dates
               ["contact_id", "event_category", "event_created_at", "contact_created_at"]]
    .copy()  # Create independent copy
)

# Filter events to ensure they occur after contact creation (data quality check)
events_df = events_df[events_df["event_created_at"] >= events_df["contact_created_at"]]

# Sort events chronologically for each contact to analyze engagement sequence
events_df = events_df.sort_values(["contact_id", "event_created_at"]).reset_index(drop=True)

print(f"‚úÖ Events table created with {events_df.shape[0]:,} records")

"""*7.DATABASE LOADING AND ANALYSIS*"""

print("\nüíæ STARTING DATABASE LOADING...")

# Create dlt pipeline for loading data into DuckDB
pipeline = dlt.pipeline(
    pipeline_name="utm_attribution_analysis",
    destination="duckdb",
    dataset_name="silver"
)

# Load contacts data into database
print("\nüîÑ LOADING CONTACTS TO DUCKDB...")
info_contacts = pipeline.run(
    contacts_df,
    table_name="contacts",
    write_disposition="replace"
)

# Load events data into database
print("\nüîÑ LOADING EVENTS TO DUCKDB...")
info_events = pipeline.run(
    events_df,
    table_name="events",
    write_disposition="replace"
)

print("‚úÖ DATA SUCCESSFULLY LOADED TO DUCKDB!")

"""*8. SEPARATED ANALYSIS VIEWS: UTM ONLY, GENERAL, AND NON-UTM*"""

# Connect to DuckDB database for analysis
conn = duckdb.connect("/content/utm_attribution_analysis.duckdb")

# =============================================================================
# VIEW 1: GENERAL ANALYSIS (ALL SOURCES)
# =============================================================================

print("\nüåê VIEW 1: GENERAL ANALYSIS - ALL SOURCES")

# General acquisition analysis - ALL sources
general_acquisition = conn.execute("""
    SELECT
        utm_source as source_utm,
        COUNT(DISTINCT contact_id) as total_contacts,
        MIN(contact_created_at) as first_acquisition,
        MAX(contact_created_at) as last_acquisition
    FROM silver.contacts
    GROUP BY utm_source
    ORDER BY total_contacts DESC
""").fetchdf()

print("üìä GENERAL ACQUISITION ANALYSIS (ALL SOURCES):")
display(general_acquisition.head(15))

# =============================================================================
# VIEW 2: UTM-ONLY ANALYSIS
# =============================================================================

print("\nüéØ VIEW 2: UTM-ONLY ANALYSIS")

# UTM-only acquisition analysis
utm_only_acquisition = conn.execute("""
    SELECT
        utm_source as source_utm,
        COUNT(DISTINCT contact_id) as total_contacts,
        MIN(contact_created_at) as first_acquisition,
        MAX(contact_created_at) as last_acquisition
    FROM silver.contacts
    WHERE LOWER(utm_source) LIKE '%utm%'
    GROUP BY utm_source
    ORDER BY total_contacts DESC
""").fetchdf()

print("üìä UTM-ONLY ACQUISITION ANALYSIS:")
display(utm_only_acquisition)

# =============================================================================
# VIEW 3: NON-UTM ANALYSIS
# =============================================================================

print("\nüìä VIEW 3: NON-UTM ANALYSIS")

# Non-UTM acquisition analysis
non_utm_acquisition = conn.execute("""
    SELECT
        utm_source as source_utm,
        COUNT(DISTINCT contact_id) as total_contacts,
        MIN(contact_created_at) as first_acquisition,
        MAX(contact_created_at) as last_acquisition
    FROM silver.contacts
    WHERE LOWER(utm_source) NOT LIKE '%utm%'
    GROUP BY utm_source
    ORDER BY total_contacts DESC
""").fetchdf()

print("üìä NON-UTM ACQUISITION ANALYSIS:")
display(non_utm_acquisition.head(15))

"""*9. INSIGHTS AND RECOMMENDATIONS*"""

# =============================================================================
# VISUALIZATION 1: GENERAL ANALYSIS DASHBOARD WITH DATA LABELS
# =============================================================================

print("\n" + "="*60)
print("üìä GENERAL ANALYSIS DASHBOARD - ALL SOURCES")
print("="*60)

def create_acquisition_dashboard_with_labels(df, title, height=800):
    """Create a comprehensive acquisition dashboard with data labels and no legends"""

    # Sort data for consistent ordering
    df_sorted = df.sort_values('total_contacts', ascending=True)

    # Create subplots with proper spacing
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            f'A. Acquisition Volume - {title}',
            f'B. Percentage Distribution - {title}',
            f'C. Duration Analysis - {title}',
            f'D. Cumulative Distribution - {title}'
        ),
        specs=[
            [{"type": "bar"}, {"type": "bar"}],
            [{"type": "scatter"}, {"type": "scatter"}]
        ],
        vertical_spacing=0.15,
        horizontal_spacing=0.1
    )

    # Chart 1: Horizontal bar chart with data labels
    top_15_sources = df_sorted.tail(15)  # Get top 15 sources

    fig.add_trace(
        go.Bar(
            y=top_15_sources['source_utm'],
            x=top_15_sources['total_contacts'],
            orientation='h',
            marker=dict(
                color=top_15_sources['total_contacts'],
                colorscale='Viridis',
                showscale=False
            ),
            hovertemplate=(
                "<b>%{y}</b><br>" +
                "Contacts: %{x:,}<br>" +
                "<extra></extra>"
            ),
            text=top_15_sources['total_contacts'].apply(lambda x: f"{x:,}"),  # Data labels
            textposition='outside',  # Position labels outside bars
            textfont=dict(size=10, color='black'),  # Label styling
            name=""
        ),
        row=1, col=1
    )

    # Chart 2: Percentage distribution with data labels
    df_sorted['percentage'] = (df_sorted['total_contacts'] / df_sorted['total_contacts'].sum()) * 100
    top_10_sources = df_sorted.tail(10)

    fig.add_trace(
        go.Bar(
            y=top_10_sources['source_utm'],
            x=top_10_sources['percentage'],
            orientation='h',
            marker=dict(
                color=top_10_sources['percentage'],
                colorscale='Plasma',
                showscale=False
            ),
            hovertemplate=(
                "<b>%{y}</b><br>" +
                "Percentage: %{x:.1f}%<br>" +
                "Contacts: %{customdata:,}<br>" +
                "<extra></extra>"
            ),
            customdata=top_10_sources['total_contacts'],
            text=top_10_sources['percentage'].apply(lambda x: f"{x:.1f}%"),  # Data labels
            textposition='outside',
            textfont=dict(size=10, color='black'),
            name=""
        ),
        row=1, col=2
    )

    # Chart 3: Duration analysis with data labels
    df['operation_days'] = (df['last_acquisition'] - df['first_acquisition']).dt.days
    df_temporal = df[df['operation_days'] > 0].head(10)

    if not df_temporal.empty:
        fig.add_trace(
            go.Scatter(
                x=df_temporal['total_contacts'],
                y=df_temporal['operation_days'],
                mode='markers+text',  # Add text mode for labels
                marker=dict(
                    size=20,  # Increased size for better visibility
                    color=df_temporal['operation_days'],
                    colorscale='Viridis',
                    showscale=False,  # Remove colorbar
                    line=dict(width=2, color='DarkSlateGrey')
                ),
                text=df_temporal['total_contacts'].apply(lambda x: f"{x:,}"),  # Data labels
                textposition='middle right',  # Position labels to the right
                textfont=dict(size=9, color='black'),
                hovertemplate=(
                    "<b>%{customdata}</b><br>" +
                    "Contacts: %{x:,}<br>" +
                    "Days Operating: %{y}<br>" +
                    "<extra></extra>"
                ),
                customdata=df_temporal['source_utm'],
                name=""
            ),
            row=2, col=1
        )

    # Chart 4: Cumulative distribution with data labels
    df_cumulative = df_sorted.sort_values('total_contacts', ascending=False).copy()
    df_cumulative['cumulative'] = df_cumulative['total_contacts'].cumsum()
    df_cumulative['cumulative_percentage'] = (df_cumulative['cumulative'] / df_cumulative['total_contacts'].sum()) * 100
    top_10_cumulative = df_cumulative.head(10)

    fig.add_trace(
        go.Scatter(
            x=list(range(len(top_10_cumulative))),
            y=top_10_cumulative['cumulative_percentage'],
            mode='lines+markers+text',  # Add text mode for labels
            line=dict(width=3, color='red'),
            marker=dict(size=10, color='red'),
            text=top_10_cumulative['cumulative_percentage'].apply(lambda x: f"{x:.1f}%"),  # Data labels
            textposition='top center',  # Position labels above points
            textfont=dict(size=9, color='red'),
            hovertemplate=(
                "Rank: %{x+1}<br>" +
                "Source: <b>%{customdata}</b><br>" +
                "Cumulative: %{y:.1f}%<br>" +
                "<extra></extra>"
            ),
            customdata=top_10_cumulative['source_utm'],
            name=""
        ),
        row=2, col=2
    )

    # Update layout with proper spacing and no legend
    fig.update_layout(
        title_text=f"Acquisition Analysis Dashboard - {title}",
        title_x=0.5,
        title_font=dict(size=20, family="Arial", color="#2C3E50"),
        height=height,
        width=1200,
        showlegend=False,  # Remove legend completely
        template="plotly_white",
        font=dict(family="Arial", size=11),
        margin=dict(t=100, b=80, l=80, r=80)
    )

    # Update axes with proper sizing
    fig.update_xaxes(title_text="Number of Contacts", row=1, col=1)
    fig.update_yaxes(title_text="Source", row=1, col=1)

    fig.update_xaxes(title_text="Percentage (%)", row=1, col=2)
    fig.update_yaxes(title_text="Source", row=1, col=2)

    if not df_temporal.empty:
        fig.update_xaxes(title_text="Total Contacts", row=2, col=1)
        fig.update_yaxes(title_text="Days in Operation", row=2, col=1)

    fig.update_xaxes(title_text="Source Ranking", row=2, col=2)
    fig.update_yaxes(title_text="Cumulative %", row=2, col=2)

    return fig

# Create separate dashboards for each view with data labels
fig_general = create_acquisition_dashboard_with_labels(general_acquisition, "All Sources", 800)
fig_general.show()

# =============================================================================
# VISUALIZATION 2: UTM-ONLY DASHBOARD WITH DATA LABELS
# =============================================================================

print("\n" + "="*50)
print("üéØ UTM-ONLY ACQUISITION DASHBOARD")
print("="*50)

if not utm_only_acquisition.empty:
    fig_utm = create_acquisition_dashboard_with_labels(utm_only_acquisition, "UTM Sources Only", 600)
    fig_utm.show()
else:
    print("‚ö†Ô∏è No UTM sources found in the data")

# =============================================================================
# VISUALIZATION 3: NON-UTM DASHBOARD WITH DATA LABELS
# =============================================================================

print("\n" + "="*50)
print("üìä NON-UTM ACQUISITION DASHBOARD")
print("="*50)

if not non_utm_acquisition.empty:
    fig_non_utm = create_acquisition_dashboard_with_labels(non_utm_acquisition, "Non-UTM Sources", 600)
    fig_non_utm.show()
else:
    print("‚ö†Ô∏è No non-UTM sources found in the data")

# =============================================================================
# ENGAGEMENT ANALYSIS WITH DATA LABELS
# =============================================================================

print("\n" + "="*60)
print("üìà ENGAGEMENT ANALYSIS WITH DATA LABELS")
print("="*60)

# Engagement metrics for UTM sources only
engagement_metrics_utm = conn.execute("""
    WITH contact_events AS (
        SELECT
            c.contact_id as contact_id,
            c.utm_source as source_utm,
            c.contact_created_at as contact_creation_date,
            COUNT(e.event_created_at) as total_events,
            COUNT(DISTINCT e.event_category) as unique_event_types,
            MIN(e.event_created_at) as first_event_date,
            MAX(e.event_created_at) as last_event_date
        FROM silver.contacts c
        LEFT JOIN silver.events e ON c.contact_id = e.contact_id
        WHERE LOWER(c.utm_source) LIKE '%utm%'
        GROUP BY c.contact_id, c.utm_source, c.contact_created_at
    )
    SELECT
        source_utm,
        COUNT(contact_id) as acquired_contacts,
        SUM(CASE WHEN total_events > 0 THEN 1 ELSE 0 END) as contacts_with_events,
        ROUND(SUM(CASE WHEN total_events > 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(contact_id), 2) as engagement_rate_percent,
        AVG(total_events) as avg_events_per_contact
    FROM contact_events
    GROUP BY source_utm
    ORDER BY engagement_rate_percent DESC
""").fetchdf()

print("üìä UTM-ONLY ENGAGEMENT METRICS:")
display(engagement_metrics_utm)

# Create engagement charts with data labels
if not engagement_metrics_utm.empty:
    # Engagement Rate Chart with Data Labels
    fig_engagement_rate = go.Figure()

    fig_engagement_rate.add_trace(go.Bar(
        x=engagement_metrics_utm['source_utm'].head(10),
        y=engagement_metrics_utm['engagement_rate_percent'].head(10),
        marker_color='lightblue',
        text=engagement_metrics_utm['engagement_rate_percent'].head(10).apply(lambda x: f"{x:.1f}%"),  # Data labels
        textposition='outside',
        textfont=dict(size=11, color='black'),
        hovertemplate=(
            "<b>%{x}</b><br>" +
            "Engagement Rate: %{y:.1f}%<br>" +
            "Contacts: %{customdata}<br>" +
            "<extra></extra>"
        ),
        customdata=engagement_metrics_utm['acquired_contacts'].head(10),
        name=""
    ))

    fig_engagement_rate.update_layout(
        title="Engagement Rate by UTM Source (Top 10)",
        xaxis_title="UTM Source",
        yaxis_title="Engagement Rate (%)",
        showlegend=False,  # Remove legend
        template="plotly_white",
        height=500,
        width=800
    )

    fig_engagement_rate.update_xaxes(tickangle=45)
    fig_engagement_rate.show()

    # Average Events Chart with Data Labels
    fig_avg_events = go.Figure()

    fig_avg_events.add_trace(go.Bar(
        x=engagement_metrics_utm['source_utm'].head(10),
        y=engagement_metrics_utm['avg_events_per_contact'].head(10),
        marker_color='lightgreen',
        text=engagement_metrics_utm['avg_events_per_contact'].head(10).apply(lambda x: f"{x:.2f}"),  # Data labels
        textposition='outside',
        textfont=dict(size=11, color='black'),
        hovertemplate=(
            "<b>%{x}</b><br>" +
            "Avg Events: %{y:.2f}<br>" +
            "Contacts: %{customdata}<br>" +
            "<extra></extra>"
        ),
        customdata=engagement_metrics_utm['acquired_contacts'].head(10),
        name=""
    ))

    fig_avg_events.update_layout(
        title="Average Events per Contact by UTM Source (Top 10)",
        xaxis_title="UTM Source",
        yaxis_title="Average Events",
        showlegend=False,  # Remove legend
        template="plotly_white",
        height=500,
        width=800
    )

    fig_avg_events.update_xaxes(tickangle=45)
    fig_avg_events.show()

# =============================================================================
# CONCENTRATION ANALYSIS WITH DATA LABELS
# =============================================================================

print("\n" + "="*50)
print("üìä CONCENTRATION ANALYSIS WITH DATA LABELS")
print("="*50)

# Create concentration analysis for each view
def create_concentration_chart(df, title):
    if df.empty:
        return None

    top_8_sources = df.head(8)

    fig = go.Figure()

    fig.add_trace(go.Bar(
        x=top_8_sources['source_utm'],
        y=top_8_sources['total_contacts'],
        marker_color='coral',
        text=top_8_sources['total_contacts'].apply(lambda x: f"{x:,}"),  # Data labels
        textposition='outside',
        textfont=dict(size=11, color='black'),
        hovertemplate=(
            "<b>%{x}</b><br>" +
            "Contacts: %{y:,}<br>" +
            "<extra></extra>"
        ),
        name=""
    ))

    # Calculate concentration metrics
    total_contacts = df['total_contacts'].sum()
    top_5_percentage = df.nlargest(5, 'total_contacts')['total_contacts'].sum() / total_contacts * 100

    fig.update_layout(
        title=f"{title}<br><sub>Total Sources: {len(df)} | Top 5 Concentration: {top_5_percentage:.1f}%</sub>",
        xaxis_title="Source",
        yaxis_title="Number of Contacts",
        showlegend=False,  # Remove legend
        template="plotly_white",
        height=500,
        width=800
    )

    fig.update_xaxes(tickangle=45)
    return fig

# Create concentration charts for each view
if not general_acquisition.empty:
    fig_conc_general = create_concentration_chart(general_acquisition, "Acquisition Concentration - All Sources")
    if fig_conc_general:
        fig_conc_general.show()

if not utm_only_acquisition.empty:
    fig_conc_utm = create_concentration_chart(utm_only_acquisition, "Acquisition Concentration - UTM Sources")
    if fig_conc_utm:
        fig_conc_utm.show()

if not non_utm_acquisition.empty:
    fig_conc_non_utm = create_concentration_chart(non_utm_acquisition, "Acquisition Concentration - Non-UTM Sources")
    if fig_conc_non_utm:
        fig_conc_non_utm.show()

# =============================================================================
# COMPARISON SUMMARY
# =============================================================================

print("\n" + "="*60)
print("üìã COMPARISON SUMMARY ACROSS VIEWS")
print("="*60)

# Calculate summary statistics for each view
def calculate_view_summary(df, view_name):
    if df.empty:
        return {
            'view': view_name,
            'total_sources': 0,
            'total_contacts': 0,
            'avg_contacts_per_source': 0,
            'concentration_top5': 0
        }

    total_contacts = df['total_contacts'].sum()
    top_5_percentage = df.nlargest(5, 'total_contacts')['total_contacts'].sum() / total_contacts * 100 if total_contacts > 0 else 0

    return {
        'view': view_name,
        'total_sources': len(df),
        'total_contacts': total_contacts,
        'avg_contacts_per_source': df['total_contacts'].mean(),
        'concentration_top5': top_5_percentage
    }

# Create comparison summary
summary_comparison = [
    calculate_view_summary(general_acquisition, "All Sources"),
    calculate_view_summary(utm_only_acquisition, "UTM Only"),
    calculate_view_summary(non_utm_acquisition, "Non-UTM")
]

summary_df = pd.DataFrame(summary_comparison)
print("üìä COMPARISON SUMMARY:")
display(summary_df)

# =============================================================================
# FINAL INSIGHTS
# =============================================================================

print("\n" + "="*60)
print("üí° KEY INSIGHTS AND RECOMMENDATIONS")
print("="*60)

# Generate insights based on the analysis
if not general_acquisition.empty:
    top_source = general_acquisition.iloc[0]
    print(f"üèÜ TOP PERFORMING SOURCE OVERALL:")
    print(f"   ‚Ä¢ {top_source['source_utm']}: {top_source['total_contacts']:,} contacts")

if not utm_only_acquisition.empty:
    utm_top = utm_only_acquisition.iloc[0]
    print(f"\nüéØ TOP UTM SOURCE:")
    print(f"   ‚Ä¢ {utm_top['source_utm']}: {utm_top['total_contacts']:,} contacts")

    # UTM concentration analysis
    utm_concentration = utm_only_acquisition.nlargest(3, 'total_contacts')['total_contacts'].sum() / utm_only_acquisition['total_contacts'].sum() * 100
    print(f"   ‚Ä¢ Top 3 UTM sources represent {utm_concentration:.1f}% of UTM traffic")

if not non_utm_acquisition.empty:
    non_utm_top = non_utm_acquisition.iloc[0]
    print(f"\nüìä TOP NON-UTM SOURCE:")
    print(f"   ‚Ä¢ {non_utm_top['source_utm']}: {non_utm_top['total_contacts']:,} contacts")

# Strategic recommendations
print(f"\nüéØ STRATEGIC RECOMMENDATIONS:")
print(f"1. üìà FOCUS on top-performing UTM sources for scalable growth")
print(f"2. üîç INVESTIGATE high-performing non-UTM sources for new opportunities")
print(f"3. üéØ OPTIMIZE underperforming sources with A/B testing")
print(f"4. üìä CONTINUOUSLY monitor source performance across all categories")

# Close database connection
conn.close()

print("\n" + "="*60)
print("‚úÖ ANALYSIS WITH DATA LABELS COMPLETED SUCCESSFULLY!")
print("="*60)